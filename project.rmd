---
title: "Bayesain Project"
author: "Ammar Hasan"
date: "18 February 2019"
toc: T
output: pdf_document
---

```{r setup, include=FALSE}

library(knitr)
library(rjags)
library(plyr)

opts_chunk$set(echo = TRUE)

```

\newpage

# Introduction

This report summaries the Bayesian analysis of the Reisby data-set. The Reisby data-set is based on a 5 week (first placebo) psychiatric study which investigates response of depressed patients to IMI. The Bayesian analysis will investigate how the drug affects depression. 

# Data Setup and Cleaning

## Data

The data is loaded from a .Rdata file containing the Riesby data-set introduced in the introduction.

```{r data, echo=FALSE}

load("Reisby.RData")
Reisby = as.data.frame(Reisby)

```

## Data Exploration

### Correlation (Pairs Plot)

```{r pairs, echo = FALSE}

kable(cor(Reisby[,-1])) # no id

```

The strongest correlations are negative and weak-moderate, and occur between Hamilton index with week and DMI, and rather obvious as the increase in blood concentration of the antidepressant would alleviate depression over weeks of treatment. Most other correlations are weak to very weak.

\newpage

### Graphical Summaries 

#### Hamilton Scores

```{r ham-hist, echo=FALSE}

hist(Reisby$hd, main = "Hamilton Index Scores Histogram",
     xlab = "Hamilton Score")

```

Most Hamilton scores are above 20 (i.e. moderate and severe depression dominates against mild and normal)

#### Sex

```{r sex-hist, echo=FALSE}

# Replace x axis here with male/female axis

hist(Reisby$female, main = "Sex Histogram",
     xlab = "Sex", xaxt = "n", ylim = c(0, 200)) 
axis(1, at=0:1, labels=c("Male","Female"))

```

The female test subjects are overwhelmingly higher than the males (nearly double!).

\newpage

#### Endogenous vs Reactive (Depression Type)

```{r dep-type-hist, echo=FALSE}

hist(Reisby$reactive_depression, main = "Depression Type Histogram",
     xlab = "Depression Type", xaxt = "n", ylim = c(0, 150)) 
axis(1, at=0:1, labels=c("Endogenous","Reactive"))

```

Most Depression cases in the test population are Endogenous (i.e. not a reaction to an environmental event).

\newpage

#### DMI and IMI Concentrations

```{r dmi-imi-boxplot, echo=FALSE}

boxplot(Reisby$lnimi, Reisby$lndmi, main = "IMI and DMI Concentration Distributions",
        ylab = "Log Concentraion") 
axis(1, at=0:2, labels=c("IMI","IMI", "DMI")) # uses 3 labels due to strange bug

```

Generally tight distribution *especially around 25% to 75%) for both with little outlines with both distribution looking nearly identical sans the shift up with DMI. It seems that after being processed as DMI, the concentration of the antidepressant in the blood increases.

# Models

## General Diagnostic notes

For diagnostics, all models start with 1000 samples with no thinning and no burn in discard. The models are then diagnosed using the following techniques/measurements:

* Trace plots: Used to observe general mixing via parameter values, aim to have random caterpillar like shape.
* Effective Size: Used to observe correlation effect on data by check how effective the samples are compared to independent data. Aim to have them consistently close and somewhere near half the samples.
* Gelman Rubin: Used to check convergence of model through comparing chains, aim to have value of 1 or very close to 1.
* Autocolleration Plots: Useful to check need for thinning and thinning effect by checking sample correlations between each other, lags presented here should remain consistently low.

## Linear Regression

In this method a relationship between a predictor x and a response y is established through a Linear Function with a coefficient the predictors and the intercept. 

### Data Cleaning and Preparation

#### Weeks 

```{r clean-weeks, echo = FALSE}

placebo = c(ifelse(Reisby$week == 0, 1, 0))
  
```


There are going to be two weeks variables, the initial week number for Autoregression and Gaussian Proccess, and another or on whether it is a placebo week or not for linear regression. 

#### Standardisation and Predictor Separation

The response (Hamilton index) and predictors (everything else) are separated for the modelling stage. Predictors (x) for linear regression use placebo indicator for weeks instead of actual weeks (not a time series). All non indicator predictors are standardised using a to ensure a fair impacts between variables and easier uninformative prior selection.

```{r pred-sep, echo = FALSE}

x = Reisby[,-2:-3]
x$placebo = placebo
y = Reisby[,2]

```


```{r std, echo = FALSE}

scaled_dmi = (Reisby$lnimi - mean(Reisby$lnimi))/sd(Reisby$lnimi)
scaled_imi = (Reisby$lndmi - mean(Reisby$lndmi))/sd(Reisby$lndmi)

lin_x_scaled = x[, -2:-3]

lin_x_scaled$scaled_dmi = scaled_dmi
lin_x_scaled$scaled_imi = scaled_imi

```

### Basic Multiple 

Multiple Linear Regression is a simple extension of Linear regression where multiple predictors each with their own coefficients are introduced.

#### Modelling and Diagnostics

```{r lin-mod, echo = FALSE}

data = list(x = lin_x_scaled[,-1], y = y,
            n = nrow(lin_x_scaled), 
            p = ncol(lin_x_scaled[,-1]))

# JAG model (as a string)

model_string = "
model {
  b0 ~ dnorm(0, 1E-6) 

  for (j in 1:p) {
    b[j] ~ dnorm(0, 1E-6)
  }

  tau ~ dgamma(0.001, 0.001)
  sd = pow(tau, -0.5)

  for (i in 1:n) {
    y[i] ~ dnorm(mu[i], tau) T(0,60)
    mu[i] = b0 + inprod(b, x[i,])
  }

}
"

# model construction and sampling

model_tc = textConnection(model_string)
model = jags.model(model_tc, data = data, n.chains = 4)

update(model, n.iter = 1000)
samples = coda.samples(model, 
                       variable.names = c("b0","sd","b"),
                       n.iter = 1000 * 13, thin = 13)
lin_dic = dic.samples(model, 
                       variable.names = c("b0","sd","b"),
                       n.iter = 1000 * 13, thin = 13)

```

The model was constructed in JAGS using a multiple linear regression based on a normal model using uninformative priors gamma (0.001, 0.001) for precision and N(0, 1E-6) for coefficients. The final model is truncated to values between 0 and 60 since this is the range for the Hamilton scores.

The model was difficult to configure as $b0$, $b1$ and $b2$ had mixing issues observable in the trace plot, which only improved after a burn in and thinning. The level thinning was decided on using the autocorrelation plot which showed a need of thinning every 10th value for $b0$! When it came to other metrics, effective size reflected the high correlation with the $beta$ parameters - around 490 for $B1$ and 404 for $b0$, more than half of other parameters, but siginifcantly improved after thinning - most parameters around 6840-8380 Convergence was not a problem according to gelman diag diagnostics which remained near or at 1.0.


#### Summaries

```{r lin-mod-sum}

hpd = HPDinterval(samples)

kable(hpd[[1]])

```

#### Predictions

### Term Interactions

#### Modelling and Diagnostics

```{r lin-mod-term}

data = list(x = lin_x_scaled[,-1], y = y,
            n = nrow(lin_x_scaled), 
            p = ncol(lin_x_scaled[,-1]))

# JAG model (as a string)

model_string = "
model {
  b0 ~ dnorm(0, 1E-6) 
  c ~ dnorm(0, 1E-6)

  for (j in 1:p) {
    b[j] ~ dnorm(0, 1E-6)
  }

  tau ~ dgamma(0.001, 0.001)
  sd = pow(tau, -0.5)

  for (i in 1:n) {
    y[i] ~ dnorm(mu[i], tau) T(0,60)
    mu[i] = b0 + inprod(b, x[i,]) + c * x[i, 4] * x[i, 5]
  }

}
"

# model construction and sampling

model_tc = textConnection(model_string)
model = jags.model(model_tc, data = data, n.chains = 4)

update(model, n.iter = 1000)
samples = coda.samples(model, 
                       variable.names = c("b0","sd","b", "c"),
                       n.iter = 1000 * 13, thin = 13)
dic_terms = coda.samples(model, 
                       variable.names = c("b0","sd","b", "c"),
                       n.iter = 1000 * 13, thin = 13)

```



#### Summaries

Not everyone was measured every week.

### Hierarchical

#### Modelling and Diagnostics


```{r lin-mod-hie}

data = list(x = lin_x_scaled[,-1], y = y,
            n = nrow(lin_x_scaled), 
            p = ncol(lin_x_scaled[,-1]),
            n_subjects = length(unique(lin_x_scaled[,1])),
            subject = lin_x_scaled$id)

# JAG model (as a string)

model_string = "
model {
  b0 ~ dnorm(0, 1E-6) 

  for (j in 1:p) {
    b[j] ~ dnorm(0, 1E-6)
  }

  for (k in 1:n_subjects) {
    c[k] ~ dnorm(0, tau_hier)
  }

  tau ~ dgamma(0.001, 0.001)
  tau_hier ~ dgamma(0.001, 0.001)
  sd = pow(tau, -0.5)
  sd_hier = pow(tau_hier, -0.5)

  for (i in 1:n) {
    y[i] ~ dnorm(mu[i], tau) T(0,60)
    mu[i] = b0 + inprod(b, x[i,]) + c[subjects[i]]
  }

}
"

# model construction and sampling

model_tc = textConnection(model_string)
model = jags.model(model_tc, data = data, n.chains = 4)

update(model, n.iter = 1000)
samples = coda.samples(model, 
                       variable.names = c("b0","sd","b", "sd_hier",
                                          "c"),
                       n.iter = 1000 * 13, thin = 13)
dic_terms = coda.samples(model, 
                       variable.names = c("b0","sd","b", "sd_hier",
                                          "c"),
                       n.iter = 1000 * 13, thin = 13)

```



#### Summaries


## Auto Regression

### Data Cleaning and Perparation
### User Matricies

```{r user-mat}

scaled_dmi = (Reisby$lnimi - mean(Reisby$lnimi))/sd(Reisby$lnimi)
scaled_imi = (Reisby$lndmi - mean(Reisby$lndmi))/sd(Reisby$lndmi)

SReisby = Reisby[, -4:-5]
SReisby$scaled_dmi = scaled_dmi
SReisby$scaled_imi = scaled_imi

weekCount = count(SReisby, "id")
full_ids = subset(weekCount, freq == max(SReisby$week) + 1)
sub_SReisby = subset(SReisby, id %in% full_ids$id)
ids = unique(sub_SReisby$id)


score_mat = matrix(nrow = length(ids), 
                  ncol = max(sub_SReisby$week) + 1) 
imi_mat = matrix(nrow = length(ids), 
                  ncol = max(sub_SReisby$week) + 1) 
dmi_mat = matrix(nrow = length(ids), 
                  ncol = max(sub_SReisby$week) + 1) 
female_mat = matrix(nrow = length(ids), 
                  ncol = max(sub_SReisby$week) + 1) 
dep_mat = matrix(nrow = length(ids), 
                  ncol = max(sub_SReisby$week) + 1) 

for(i in 1:length(ids)){
  for(j in 0:max(sub_SReisby$week)){
    score_mat[i, j+1] = subset(sub_SReisby, id == ids[i] & week == j)$hd
    imi_mat[i, j+1] = subset(sub_SReisby, id == ids[i] & week == j)$scaled_imi
    dmi_mat[i, j+1] = subset(sub_SReisby, id == ids[i] & week == j)$scaled_dmi
    female_mat[i, j+1] = subset(sub_SReisby, id == ids[i] & week == j)$female
    dep_mat[i, j+1] = subset(sub_SReisby, id == ids[i] & week == j)$reactive_depression

  }
}

```


### AR(1)

```{r ar1-model}

### Modelling (AR(1))

data = list(y = score_mat, x1 = imi_mat, x2 = dmi_mat, x3 = female_mat,
            x4 = dep_mat, nrow = nrow(score_mat), ncol = ncol(score_mat),
            p = 4)

model_string = "
model {

  for (j in 1:p) {
    b[j] ~ dnorm(0, 1E-6) 
  }

  for (i in 1:nrow) {
    for(k in 2:ncol){
      y[i,k] ~ dnorm (mu + a1 * (y[i, (k-1)] - mu), tau) T(0,60)
    }
  }

  mu ~ dnorm (0, 1E-6) T(0,60)
  a1 ~ dnorm (0, 0.35^-2) 
  tau ~ dgamma (0.001, 0.001)
  sd = pow (tau , -0.5)
}
"

model_tc = textConnection(model_string)
model = jags.model(model_tc, data = data, n.chains = 4)

## Sampling

update(model, n.iter = 4000)
samples = coda.samples(model, variable.names = c("mu", "a1", "sd",
                                                 "b"),
                             n.iter = 1000 * 14, thin = 14)

```


#### Modelling and Diagnostics

#### Summaries

### Investigating Ideal Degree (finding p)

#### Modelling and Diagnostics

#### Summaries


# Conclusion


\newpage

# Appendix

## Abbrevations 

IMI - antidepressant drug imipramine 

DMI - desmethylimipramlne (Processed IMI)

AR - Auto Regressive Models

AR(1) - Auto Regressive Models (1st Degree)

AR(2) - Auto Regressive Models (2nd Degree)

JAGS - Just Another Gibbs Sampler MCMC based Bayesian sampler 

MCMC - Markov chain Monte Carlo


## Code

### Data

#### Data Loading

```{r, ref.label= 'data', echo=TRUE, eval=FALSE}

```

#### Correlation (Pairs Plot)

```{r, ref.label= 'pairs', echo=TRUE, eval=FALSE}

```

#### Hamilton Scores Histogram

```{r, ref.label= 'ham-hist', echo=TRUE, eval=FALSE}

```

#### Sex Histogram

```{r, ref.label= 'sex-hist', echo=TRUE, eval=FALSE}

```

#### Depression Type Histogram

```{r, ref.label= 'dep-type-hist', echo=TRUE, eval=FALSE}

```

#### DMI and IMI Boxplot

```{r, ref.label= 'dmi-imi-boxplot', echo=TRUE, eval=FALSE}

```

#### Weeks Cleaining

```{r, ref.label= 'weeks-cleaning', echo=TRUE, eval=FALSE}

```

### Models

#### Basic Multiple Linear Regression

```{r, ref.label= 'lin-mod', echo=TRUE, eval=FALSE}

```

