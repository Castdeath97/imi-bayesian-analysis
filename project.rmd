---
title: "Bayesain Project"
author: "Ammar Hasan"
date: "18 February 2019"
toc: T
output: pdf_document
---

```{r setup, include=FALSE}

library(knitr)
library(rjags)

opts_chunk$set(echo = TRUE)

```

\newpage

# Introduction

This report summaries the Bayesian analysis of the Reisby data-set. The Reisby data-set is based on a 5 week (first placebo) psychiatric study which investigates response of depressed patients to IMI. The Bayesian analysis will investigate how the drug affects depression. 

# Data Setup and Cleaning

## Data

The data is loaded from a .Rdata file containing the Riesby data-set introduced in the introduction.

```{r data, echo=FALSE}

load("Reisby.RData")
Reisby = as.data.frame(Reisby)

```

## Data Exploration

### Correlation (Pairs Plot)

```{r pairs, echo = FALSE}

kable(cor(Reisby[,-1])) # no id

```

The strongest correlations are negative and weak-moderate, and occur between Hamilton index with week and DMI, and rather obvious as the increase in blood concentration of the antidepressant would alleviate depression over weeks of treatment. Most other correlations are weak to very weak.

\newpage

### Graphical Summaries 

#### Hamilton Scores

```{r ham-hist, echo=FALSE}

hist(Reisby$hd, main = "Hamilton Index Scores Histogram",
     xlab = "Hamilton Score")

```

Most Hamilton scores are above 20 (i.e. moderate and severe depression dominates against mild and normal)

#### Sex

```{r sex-hist, echo=FALSE}

# Replace x axis here with male/female axis

hist(Reisby$female, main = "Sex Histogram",
     xlab = "Sex", xaxt = "n", ylim = c(0, 200)) 
axis(1, at=0:1, labels=c("Male","Female"))

```

The female test subjects are overwhelmingly higher than the males (nearly double!).

\newpage

#### Endogenous vs Reactive (Depression Type)

```{r dep-type-hist, echo=FALSE}

hist(Reisby$reactive_depression, main = "Depression Type Histogram",
     xlab = "Depression Type", xaxt = "n", ylim = c(0, 150)) 
axis(1, at=0:1, labels=c("Endogenous","Reactive"))

```

Most Depression cases in the test population are Endogenous (i.e. not a reaction to an environmental event).

\newpage

#### DMI and IMI Concentrations

```{r dmi-imi-boxplot, echo=FALSE}

boxplot(Reisby$lnimi, Reisby$lndmi, main = "IMI and DMI Concentration Distributions",
        ylab = "Log Concentraion") 
axis(1, at=0:2, labels=c("IMI","IMI", "DMI")) # uses 3 labels due to strange bug

```

Generally tight distribution *especially around 25% to 75%) for both with little outlines with both distribution looking nearly identical sans the shift up with DMI. It seems that after being processed as DMI, the concentration of the antidepressant in the blood increases.

## Data Cleaning and Preparation

### Weeks 

```{r clean-weeks, echo = FALSE}

placebo = c(ifelse(Reisby$week == 0, 1, 0))
  
```


There are going to be two weeks variables, the initial week number for Autoregression and Gaussian Proccess, and another or on whether it is a placebo week or not for linear regression. 

### Standardisation and Predictor Separation

The response (Hamilton index) and predictors (everything else) are separated for the modelling stage. Two sets of predictors (x) are used, one for linear regression (uses placebo indicator for weeks) and one for other time series based methods (uses normal weeks). All non indicator variables are standardised using a to ensure a fair impacts between variables and easier uninformative prior selection.

```{r pred-sep, echo = FALSE}

lin_x = Reisby[,-1:-3]
lin_x$placebo = placebo
x = Reisby[,-1:-2]
y = Reisby[,1]

```


```{r std, echo = FALSE}

y_scaled = (y - mean(y))/sd(y)
scaled_dmi = (Reisby$lnimi - mean(Reisby$lnimi))/sd(Reisby$lnimi)
scaled_imi = (Reisby$lndmi - mean(Reisby$lndmi))/sd(Reisby$lndmi)

x_scaled = x[, -2:-3]
lin_x_scaled = lin_x[, -1:-2]

x_scaled$scaled_dmi = scaled_dmi
x_scaled$scaled_imi = scaled_imi
lin_x_scaled$scaled_dmi = scaled_dmi
lin_x_scaled$scaled_imi = scaled_imi

```


# Models

## General Diagnostic notes

For diagnostics, all models start with 1000 samples with no thinning and no burn in discard. The models are then diagnosed using the following techniques/measurements:

* Trace plots: Used to observe general mixing via parameter values, aim to have random caterpillar like shape.
* Effective Size: Used to observe correlation effect on data by check how effective the samples are compared to independent data. Aim to have them consistently close and somewhere near half the samples.
* Gelman Rubin: Used to check convergence of model through comparing chains, aim to have value of 1 or very close to 1.
* Autocolleration Plots: Useful to check need for thinning and thinning effect by checking sample correlations between each other, lags presented here should remain consistently low.

## Linear Regression

In this method a relationship between a predictor x and a response y is established through a Linear Function with a coefficient the predictors and the intercept. 

### Basic Multiple 

Multiple Linear Regression is a simple extension of Linear regression where multiple predictors each with their own coefficients are introduced.

#### Modelling and Diagnostics

```{r lin-mod, echo = FALSE}

data = list(x = lin_x_scaled, y = y_scaled,
            n = nrow(lin_x_scaled), p = ncol(lin_x_scaled))

# JAG model (as a string)

model_string = "
model {
  b0 ~ dnorm(0, 1E-6)

  for (j in 1:p) {
    b[j] ~ dnorm(0, 1E-6)
  }

  tau ~ dgamma(0.001, 0.001)
  sd = pow(tau, -0.5)

  for (i in 1:n) {
    y[i] ~ dnorm(mu[i], tau)
    mu[i] = b0 + inprod(b, x[i,])
  }
}
"

# model construction and sampling

model_tc = textConnection(model_string)
model = jags.model(model_tc, data = data, n.chains = 4)

update(model, n.iter = 1000)
samples = coda.samples(model, 
                       variable.names = c("b0","sd","b"),
                       n.iter = 8*1500, thin = 8)

```

#### Summaries

```{r lin-sum}

```

### Term Interactions

#### Modelling and Diagnostics

#### Summaries

Not everyone was measured every week.

### Hierarchical

#### Modelling and Diagnostics

#### Summaries


## Auto Regression

### AR(1)

#### Modelling and Diagnostics

#### Summaries

### Investigating Ideal Degree (finding p)

#### Modelling and Diagnostics

#### Summaries


# Conclusion


\newpage

# Appendix

## Abbrevations 

IMI - antidepressant drug imipramine 

DMI - desmethylimipramlne (Processed IMI)

AR - Auto Regressive Models

AR(1) - Auto Regressive Models (1st Degree)

AR(2) - Auto Regressive Models (2nd Degree)

JAGS - Just Another Gibbs Sampler MCMC based Bayesian sampler 

MCMC - Markov chain Monte Carlo


## Code

### Data

#### Data Loading

```{r, ref.label= 'data', echo=TRUE, eval=FALSE}

```

#### Correlation (Pairs Plot)

```{r, ref.label= 'pairs', echo=TRUE, eval=FALSE}

```

#### Hamilton Scores Histogram

```{r, ref.label= 'ham-hist', echo=TRUE, eval=FALSE}

```

#### Sex Histogram

```{r, ref.label= 'sex-hist', echo=TRUE, eval=FALSE}

```

#### Depression Type Histogram

```{r, ref.label= 'dep-type-hist', echo=TRUE, eval=FALSE}

```

#### DMI and IMI Boxplot

```{r, ref.label= 'dmi-imi-boxplot', echo=TRUE, eval=FALSE}

```

#### Weeks Cleaining

```{r, ref.label= 'weeks-cleaning', echo=TRUE, eval=FALSE}

```

### Models

#### Basic Multiple Linear Regression

```{r, ref.label= 'lin-mod', echo=TRUE, eval=FALSE}

```

